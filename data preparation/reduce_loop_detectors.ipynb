{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# plot layout, darkgrid\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing the number of loop detectors \n",
    "For this part, we need to work with the largest dataset - cleaned, but not yet aggregated by averaging - and to make sure pandas imports it correctly, it is best to specify the dtype for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the processed data\n",
    "\n",
    "# Specify which columns to parse as dates\n",
    "parse_dates = ['timestamp']\n",
    "\n",
    "dtype_dict = {\n",
    "    'day': 'str',  \n",
    "    'interval': 'int32',  \n",
    "    'detid': 'str',  \n",
    "    'flow': 'float32',  \n",
    "    'occ': 'float32',  \n",
    "    'error': 'float32',  \n",
    "    'city': 'category',  \n",
    "    'speed': 'float32',\n",
    "    'length': 'float64',\n",
    "    'pos': 'float64',\n",
    "    'fclass': 'category',               \n",
    "    'road': 'str',\n",
    "    'limit': 'str',\n",
    "    'lanes': 'float64',\n",
    "    'linkid': 'float64',\n",
    "    'long': 'float64',\n",
    "    'lat': 'float64',\n",
    "}\n",
    "\n",
    "data = pd.read_csv('C:/DTU/Speciale/Kode/Speciale/data/processed/merged_data.csv', dtype=dtype_dict, parse_dates=parse_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute N datasets with K sampled detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all cities with less than 100 detectors from filtered_data (as specified in the paper & report)\n",
    "\n",
    "# for each city, count the number of detectors (unique detid)\n",
    "detectors_per_city = data.groupby('city')['detid'].nunique()\n",
    "\n",
    "# we ended up using only the following cities:\n",
    "cities = ['Augsburg', 'Bern', 'Bordeaux', 'Bremen', 'Darmstadt', 'Graz', 'Hamburg', 'Kassel', 'London', 'Losangeles', 'Madrid', 'Marseille', 'Santander', 'Speyer', 'Strasbourg', 'Stuttgart', 'Taipeh', 'Toronto', 'Toulouse', 'Zurich']\n",
    "\n",
    "# filter the data to only include the specified cities\n",
    "filtered_data = data[data['city'].isin(cities)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions handle the dataset creation - a specified number of detectors are sampled, averages are calculated, and this is repeated N time to create N datasets for each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample detectors for each city\n",
    "def sample_detectors(data, detectors_per_city, min_detectors=100, sample_size=25):\n",
    "    cities = detectors_per_city[detectors_per_city > min_detectors].index\n",
    "    sampled_detectors = (\n",
    "        data[data['city'].isin(cities)]\n",
    "        .groupby('city')['detid']\n",
    "        .unique()\n",
    "        .apply(lambda x: random.sample(list(x), sample_size))\n",
    "    )\n",
    "    return sampled_detectors.explode().reset_index()\n",
    "\n",
    "# Function to calculate averages\n",
    "def calculate_averages(data):\n",
    "    results = []\n",
    "    grouped = data.groupby(['city', 'day', 'interval'])\n",
    "    for (city, day, interval), group in grouped:\n",
    "        result = {\n",
    "            'city': city,\n",
    "            'day': day,\n",
    "            'interval': interval,\n",
    "            'avg_flow': group['flow'].mean(),\n",
    "            'avg_occupancy': group['occ'].mean(),\n",
    "        }\n",
    "        results.append(result)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Function to normalize data\n",
    "def normalize_data(df):\n",
    "    # scaler = MinMaxScaler()\n",
    "    df['norm_flow'] = df.groupby('city')['avg_flow'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "    df['norm_occ'] = df.groupby('city')['avg_occupancy'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "    # df[['a_flow', 'avg_occupancy']] = scaler.fit_transform(df[['avg_flow', 'avg_occupancy']])\n",
    "    return df\n",
    "\n",
    "# Function to create N datasets\n",
    "def create_datasets(data, detectors_per_city, N, min_detectors=100, sample_size=25):\n",
    "    all_datasets = []\n",
    "    for n in range(1, N + 1):\n",
    "        # Step 1: Sample detectors\n",
    "        sampled_detectors = sample_detectors(data, detectors_per_city, min_detectors, sample_size)\n",
    "        \n",
    "        # Step 2: Filter the data based on sampled detectors\n",
    "        filtered_data = data[\n",
    "            data.set_index(['city', 'detid']).index.isin(\n",
    "                sampled_detectors.set_index(['city', 'detid']).index\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Step 3: Calculate averages\n",
    "        averages = calculate_averages(filtered_data)\n",
    "        \n",
    "        # Step 4: Normalize the data\n",
    "        normalized_averages = normalize_data(averages)\n",
    "        \n",
    "        # Add the dataset number\n",
    "        normalized_averages['N'] = n\n",
    "        all_datasets.append(normalized_averages)\n",
    "    \n",
    "    # Combine all datasets into one DataFrame\n",
    "    combined_df = pd.concat(all_datasets, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample use - generate a 'collection' of 2 datasets for each city, each based on 10 randomly samapled detectors\n",
    "\n",
    "N = 2  # Number of datasets to generate\n",
    "K = 10 # Number of detectors to sample for each dataset\n",
    "combined_datasets = create_datasets(filtered_data, detectors_per_city, N, sample_size=K)\n",
    "\n",
    "# Save the combined dataframe to a CSV\n",
    "# combined_datasets.to_csv('averages_random_datasets.csv', index=False)\n",
    "combined_datasets.to_csv('C:/DTU/Speciale/Kode/Speciale/data/processed/reduced loop detectors/10_rand_det_2_times.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
