{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50a75f57",
   "metadata": {},
   "source": [
    "## Meta-learning experiments \n",
    "### Clean(er) code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b8c85fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import learn2learn as l2l\n",
    "\n",
    "# plot layout, darkgrid\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c029c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b22ec2",
   "metadata": {},
   "source": [
    "### Loading the relevant data\n",
    "For each experiment, a support set is needed. Here we use four different support sets that contain datasets of average flows and occupancy values (MFD values) calculated based on multiple sampled subsets of 10, 25, 50, and 75 detectors for the cities.  \n",
    "The query set consist of the average flows and occupancy values (MFD values) calculated based on all available detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d80ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Support DFs\n",
    "# Support df consisting of datasets from 10 sampled detectors:\n",
    "support_10 = pd.read_csv('C:/DTU/Speciale/Kode/Speciale/data/processed/reduced loop detectors/10_rand_det_30_times.csv')\n",
    "support_10 = support_10[~support_10['city'].isin(['Vilnius', 'Munich'])] # Vilnius and Munich are not included in the support set, as they only contain data for 1 day\n",
    "\n",
    "# Support df consisting of datasets from 25 sampled detectors:\n",
    "support_25 = pd.read_csv('C:/DTU/Speciale/Kode/Speciale/data/processed/reduced loop detectors/25_rand_det_20_times.csv')\n",
    "support_25 = support_25[~support_25['city'].isin(['Vilnius', 'Munich'])] # Vilnius and Munich are not included in the support set, as they only contain data for 1 day\n",
    "\n",
    "# Support df consisting of datasets from 50 sampled detectors:\n",
    "support_50 = pd.read_csv('C:/DTU/Speciale/Kode/Speciale/data/processed/reduced loop detectors/50_rand_det_20_times.csv')\n",
    "support_50 = support_50[~support_50['city'].isin(['Vilnius', 'Munich'])]\n",
    "\n",
    "# Support df consisting of datasets from 75 sampled detectors:\n",
    "support_75 = pd.read_csv('C:/DTU/Speciale/Kode/Speciale/data/processed/reduced loop detectors/75_rand_det_20_times.csv')\n",
    "support_75 = support_75[~support_75['city'].isin(['Vilnius', 'Munich'])]\n",
    "support_cities = support_75['city'].unique()\n",
    "\n",
    "# ----- Query DF\n",
    "query_df = pd.read_csv('C:/DTU/Speciale/Kode/Speciale/data/processed/averages.csv')\n",
    "# capitalize the city names\n",
    "query_df['city'] = query_df['city'].str.capitalize()\n",
    "# drop the cities that are not in the support\n",
    "query_df = query_df[query_df['city'].isin(support_cities)]\n",
    "query_df['norm_flow'] = query_df.groupby('city')['avg_flow'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "query_df['norm_occ'] = query_df.groupby('city')['avg_occupancy'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a54f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(values, feature_min, feature_max):\n",
    "    return values * (feature_max - feature_min) + feature_min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d76cc2",
   "metadata": {},
   "source": [
    "Picking from different datasets for each inner step:  \n",
    "\n",
    "Picking a dataset at random for each taskset (not for each inner step):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7559fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficTaskDataset(Dataset):\n",
    "    def __init__(self, train_groups, val_groups, k_shots=10, m_shots=5, inner_steps=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_groups (dict): Dictionary of city-wise training data.\n",
    "            val_groups (dict): Dictionary of city-wise validation data.\n",
    "            k_shots (int): Number of samples for the support set.\n",
    "            m_shots (int): Number of samples for the query set.\n",
    "            inner_steps (int): Number of inner-loop steps (tasks per city).\n",
    "        \"\"\"\n",
    "        self.train_groups = train_groups\n",
    "        self.val_groups = val_groups\n",
    "        self.k_shots = k_shots\n",
    "        self.m_shots = m_shots\n",
    "        self.inner_steps = inner_steps\n",
    "        self.cities = list(train_groups.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cities)  # Number of tasks = Number of cities\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Select city for the task\n",
    "        city = self.cities[idx]\n",
    "        train_data = self.train_groups[city]\n",
    "        val_data = self.val_groups[city]\n",
    "\n",
    "        # Select one dataset N randomly for all inner steps\n",
    "        selected_N = train_data['N'].sample(1, random_state=43).iloc[0]\n",
    "        selected_data = train_data[train_data['N'] == selected_N]\n",
    "\n",
    "        # Sample k_shots * inner_steps for support set\n",
    "        support_set = selected_data.sample(self.k_shots * self.inner_steps, random_state=43)\n",
    "        support_occupancy = torch.tensor(support_set['norm_occ'].values, dtype=torch.float32).view(-1, 1)\n",
    "        support_flow = torch.tensor(support_set['norm_flow'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        # Sample m_shots for query set from validation data\n",
    "        query_set = val_data.sample(self.m_shots, random_state=43)\n",
    "        query_occupancy = torch.tensor(query_set['norm_occ'].values, dtype=torch.float32).view(-1, 1)\n",
    "        query_flow = torch.tensor(query_set['norm_flow'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        return {\n",
    "            'city': city,\n",
    "            'support': {'occupancy': support_occupancy, 'flow': support_flow},\n",
    "            'query': {'occupancy': query_occupancy, 'flow': query_flow},\n",
    "        }\n",
    "\n",
    "# function for preparing data for the model\n",
    "def prepare_data(support_df, query_df, k_shots, m_shots, inner_steps=1):\n",
    "    # Pick two cities randomly for a test task\n",
    "    cities = support_df['city'].unique()\n",
    "    test_cities = np.random.choice(cities, 3, replace=False)\n",
    "\n",
    "    # Split the data into test and train\n",
    "    test_support_df = support_df[support_df['city'].isin(test_cities)]\n",
    "    test_query_df = query_df[query_df['city'].isin(test_cities)]\n",
    "    support_df = support_df[~support_df['city'].isin(test_cities)]\n",
    "    query_df = query_df[~query_df['city'].isin(test_cities)]\n",
    "    \n",
    "    # Group data by city for easier access\n",
    "    train_groups = {city: data for city, data in support_df.groupby('city')}\n",
    "    val_groups = {city: data for city, data in query_df.groupby('city')}\n",
    "    test_support_groups = {city: data for city, data in test_support_df.groupby('city')}\n",
    "    test_query_groups = {city: data for city, data in test_query_df.groupby('city')}\n",
    "\n",
    "    # Instantiating the datasets\n",
    "    task_dataset = TrafficTaskDataset(train_groups, val_groups, k_shots=k_shots, m_shots=m_shots, inner_steps=inner_steps)\n",
    "    test_task_dataset = TrafficTaskDataset(test_support_groups, test_query_groups, k_shots=k_shots, m_shots=m_shots, inner_steps=inner_steps)\n",
    "\n",
    "    # Wrap the task datasets in a DataLoader\n",
    "    task_loader = DataLoader(task_dataset, batch_size=1, shuffle=True)\n",
    "    test_task_loader = DataLoader(test_task_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    return task_loader, test_task_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f551f050",
   "metadata": {},
   "source": [
    "### MTPINN model for comparison\n",
    "\n",
    "The MTPINN model is used for comparison with the meta results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceec44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskPINNModel(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.0):\n",
    "        super(MultiTaskPINNModel, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob)\n",
    "        )\n",
    "        \n",
    "        # Output branch for flow prediction\n",
    "        self.flow_predictor = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        # Output branch for critical density prediction\n",
    "        self.cd_predictor = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # Normalize critical density to be within [0, 1] \n",
    "            # We move this to the forward pass to be able to clamp the value to [0.01, 0.99] for stability\n",
    "        )\n",
    "\n",
    "        # Output branch for max flow prediction\n",
    "        self.max_flow_predictor = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # Normalize max flow to be within [0, 1]\n",
    "        )\n",
    "\n",
    "        # Learnable parameter for offset\n",
    "        self.offset = nn.Parameter(torch.tensor(0.0))  # Initializing offset as a learnable parameter\n",
    "\n",
    "        # Learnable parameter for occ_scaler (scaling factor for the second parabola)\n",
    "        self.occ_scaler = nn.Parameter(torch.tensor(3.0))  # Initially set to 3, can be adjusted based on data\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "\n",
    "        flow = self.flow_predictor(features)\n",
    "\n",
    "        critical_density = self.cd_predictor(features) \n",
    "        critical_density = torch.clamp(critical_density, 0.05, 0.95) # clamp critical density toi ensure stability by avoiding extreme values\n",
    "        # critical_density = critical_density.mean(dim=0, keepdim=True)  # Averaging across batch for single prediction value\n",
    "\n",
    "        max_flow = self.max_flow_predictor(features)\n",
    "        # max_flow = max_flow.mean(dim=0, keepdim=True)  # Averaging across batch for single value\n",
    "\n",
    "        # Using the learnable offset and occ_scaler\n",
    "        offset = self.offset  # Directly use the learned offset parameter\n",
    "        occ_scaler = self.occ_scaler  # Use occ_scaler for dynamic adjustment of the second parabola's width\n",
    "\n",
    "        return flow, critical_density, max_flow, offset, occ_scaler\n",
    "\n",
    "    def compute_total_loss(self, flow, pred_flow, pred_cd, pred_max_flow, pred_offset, pred_occ_scaler, alpha=1.0):\n",
    "        # Flow MSE loss\n",
    "        mse_loss_flow = nn.functional.mse_loss(pred_flow, flow)\n",
    "\n",
    "        # Physics-informed loss (indirectly encourages cd and max flow learning)\n",
    "        physics_loss = self.physics_informed_loss(flow, pred_flow, pred_cd, pred_max_flow, pred_offset, pred_occ_scaler)\n",
    "\n",
    "        # Total loss with scaling factor for physics-informed loss\n",
    "        total_loss = mse_loss_flow + alpha * physics_loss\n",
    "        return total_loss\n",
    "\n",
    "    def physics_informed_loss(self, occupancy, flow, critical_density, max_flow, offset, occ_scaler):\n",
    "        # Define physics-informed loss: encourages the flow to fit a parabolic shape around critical density\n",
    "        mask_before_cd = occupancy <= critical_density\n",
    "        mask_after_cd = occupancy > critical_density\n",
    "\n",
    "        # Use max_flow - offset in parabolic fits\n",
    "        parabola1_a = -(max_flow - offset) / (critical_density ** 2 + 1e-6)  # Scaling factor for left parabola\n",
    "        fit1 = parabola1_a * (occupancy - critical_density) ** 2 + (max_flow - offset)\n",
    "\n",
    "        # parabola2_a = -(max_flow - offset) / ((1 - critical_density) ** 2  + 1e-6)  # Scaling factor for right parabola\n",
    "        # fit2 = parabola2_a * (occupancy - critical_density) ** 2 + (max_flow - offset)\n",
    "        # Adjusting the width of the second parabola with occ_scaler\n",
    "        # parabola2_a = -(max_flow - offset) / ((occ_scaler * (1 - critical_density)) ** 2 + 1e-6)  # Scaling factor for right parabola\n",
    "        parabola2_a = -(max_flow - offset) / (((occ_scaler - 1) * critical_density) ** 2 + 1e-6)  # Scaling factor for right parabola\n",
    "        fit2 = parabola2_a * (occupancy - critical_density) ** 2 + (max_flow - offset)\n",
    "\n",
    "        # MSE for both fits (unless no data points in a segment, then loss is 0)\n",
    "        mse_fit1 = torch.mean((flow[mask_before_cd] - fit1[mask_before_cd])**2) if torch.sum(mask_before_cd) > 0 else torch.tensor(0.0, device=flow.device)\n",
    "        mse_fit2 = torch.mean((flow[mask_after_cd] - fit2[mask_after_cd])**2) if torch.sum(mask_after_cd) > 0 else torch.tensor(0.0, device=flow.device)\n",
    "\n",
    "        # Regularization term for max flow position\n",
    "        penalty_term = torch.mean(torch.relu(flow - (max_flow)))  # Penalizing flow values higher than (max_flow)\n",
    "\n",
    "        # Rguarization term for the width of the second parabola, the occ_scaler to be between 1 and 4\n",
    "        penalty_term2 = torch.mean(torch.relu((occ_scaler - 5 * critical_density))) + torch.mean(torch.relu((2 * critical_density - occ_scaler))) * 5 # here we penalize the occ_scaler to be outside the range [1, 4]\n",
    "\n",
    "        # Regularization term: Penalize small Fit 2 region\n",
    "        total_points = torch.sum(mask_before_cd) + torch.sum(mask_after_cd)\n",
    "        percentage_fit2 = torch.sum(mask_after_cd) / (total_points + 1e-6)  # Avoid division by zero\n",
    "        threshold = 0.05  # Minimum percentage for Fit 2\n",
    "        penalty_term3 = torch.relu(threshold - percentage_fit2) * 0.5\n",
    "\n",
    "        # Encourage cd to align with peak flow region\n",
    "        peak_flow_penalty = torch.mean((flow - max_flow)**2 * torch.exp(-((occupancy - critical_density)**2) / (2 * 0.05 **2)))\n",
    "\n",
    "        # Inverse frequency weights, ignoring zero segments\n",
    "        weight1 = 1 / mask_before_cd.sum() if mask_before_cd.any() else 0.0\n",
    "        weight2 = 1 / mask_after_cd.sum() if mask_after_cd.any() else 0.0\n",
    "\n",
    "        # Normalize weights so they sum to 1, provided both segments have data\n",
    "        total_weight = weight1 + weight2\n",
    "        if total_weight > 0:\n",
    "            weight1 /= total_weight\n",
    "            weight2 /= total_weight\n",
    "\n",
    "        return weight1 * mse_fit1 + weight2 * mse_fit2 + penalty_term + peak_flow_penalty + penalty_term2 #(width of 2nd parabola + penalty_term3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd03536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for running the MTPINN model as a comparison\n",
    "def mtpinn_comparison(dropout, \n",
    "                      query_df, \n",
    "                      save_path, \n",
    "                      title_str, \n",
    "                      city, \n",
    "                      support_occupancy, \n",
    "                      support_flow, \n",
    "                      query_occupancy, \n",
    "                      query_flow, \n",
    "                      step):\n",
    "    \n",
    "    # Set up path for saving figures:\n",
    "    save_path = f'{save_path}/mtpinn_compar/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Initialize model\n",
    "    mtpinn_model = MultiTaskPINNModel(dropout_prob=dropout).to(device)\n",
    "    mse_loss = nn.MSELoss()\n",
    "\n",
    "    # Define optimizer and loss\n",
    "    optimizer = optim.Adam(mtpinn_model.parameters(), lr=0.001)\n",
    "    alpha = 1.0  # Weight for physics-informed loss\n",
    "    num_epochs = 100  # Number of training epochs\n",
    "    batch_size = 5  # Adjust batch size as needed\n",
    "\n",
    "    # store values\n",
    "    training_losses = []\n",
    "    mtpinn_params = {}\n",
    "\n",
    "    # Supervised training loop\n",
    "    mtpinn_model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = len(support_occupancy) // batch_size\n",
    "        for batch_idx in range(num_batches):\n",
    "            # Get batch data\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx + 1) * batch_size\n",
    "            occupancy_batch = support_occupancy[batch_start:batch_end].to(device)\n",
    "            flow_batch = support_flow[batch_start:batch_end].to(device)\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            pred_flow, pred_cd, pred_max_flow, pred_offset, pred_occ_scaler = mtpinn_model(occupancy_batch)\n",
    "\n",
    "            # Compute total loss\n",
    "            loss = mtpinn_model.compute_total_loss(flow_batch, pred_flow, pred_cd, pred_max_flow, pred_offset, pred_occ_scaler, alpha=alpha)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Store epoch loss\n",
    "        training_losses.append(epoch_loss / num_batches)\n",
    "\n",
    "    # Plot training loss\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "    ax.plot(training_losses)\n",
    "    fig.suptitle(\"Training Loss - \" + city)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"PI Total Loss\")\n",
    "    plt.savefig(save_path + f'/{title_str}_mtpinn_loss_{step}_{city}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate the model on the query set\n",
    "    mtpinn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        query_predictions, query_cd, query_max_flow, query_offset, query_occ_scaler = mtpinn_model(query_occupancy)\n",
    "\n",
    "        # Compute MSE loss on the query set\n",
    "        query_loss = mse_loss(query_predictions, query_flow)\n",
    "\n",
    "        # Calculate RRSE\n",
    "        true_values = query_flow.cpu().numpy()\n",
    "        predicted_values = query_predictions.cpu().numpy()\n",
    "        mean_true_values = np.mean(true_values)\n",
    "        rrse = np.sqrt(np.sum((predicted_values - true_values) ** 2) / np.sum((true_values - mean_true_values) ** 2))\n",
    "\n",
    "        # Calculate Correlation Coefficient\n",
    "        correlation_coefficient = np.corrcoef(true_values.flatten(), predicted_values.flatten())[0, 1]\n",
    "\n",
    "        # Ensure denormalized values are also NumPy arrays\n",
    "        query_predictions_denorm = denormalize(query_predictions, query_df['avg_flow'].min(), query_df['avg_flow'].max()).cpu().numpy()\n",
    "        query_flow_denorm = denormalize(query_flow, query_df['avg_flow'].min(), query_df['avg_flow'].max()).cpu().numpy()\n",
    "\n",
    "        # Denormalized MSE and RRSE\n",
    "        query_loss_denorm = np.mean((query_predictions_denorm - query_flow_denorm) ** 2)\n",
    "        rrse_denorm = np.sqrt(np.sum((query_predictions_denorm - query_flow_denorm) ** 2) / np.sum((query_flow_denorm - query_flow_denorm.mean()) ** 2))\n",
    "        correlation_coefficient_denorm = np.corrcoef(query_flow_denorm.flatten(), query_predictions_denorm.flatten())[0, 1]\n",
    "\n",
    "        # Store the parameters\n",
    "        for key, value in{\n",
    "            'city': city,\n",
    "            'cd': query_cd.mean().item(),  # Convert scalar tensor to float\n",
    "            'max_flow': query_max_flow.max().item(),  # Convert scalar tensor to float\n",
    "            'offset': query_offset.mean().item(),  # Convert scalar tensor to float\n",
    "            'occ_scaler': query_occ_scaler.mean().item(),  # Convert scalar tensor to float\n",
    "            'test_mse': query_loss.item(),  # Convert scalar tensor to float,\n",
    "            'rrse': rrse,\n",
    "            'correlation': correlation_coefficient,\n",
    "            'test_mse_denorm': query_loss_denorm.item(),\n",
    "            'rrse_denorm': rrse_denorm,\n",
    "            'correlation_denorm': correlation_coefficient_denorm\n",
    "        }.items():\n",
    "            mtpinn_params.setdefault(key, []).append(value)\n",
    "\n",
    "        # Print the query loss\n",
    "        print(f\"Test Loss: {query_loss.item():.4f}\")\n",
    "\n",
    "        # Plot the query predictions\n",
    "        # specify linearly spaced occupancy values for plotting the prediction\n",
    "        occupancy_values = torch.linspace(0, 1, 100).unsqueeze(1)\n",
    "        line_preds, _, _, _, _ = mtpinn_model(occupancy_values)\n",
    "        city_data = query_df[query_df['city'] == city]\n",
    "        \n",
    "        mse_text = f\"MSE: {query_loss.item():.4f}\"\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(8, 5))\n",
    "        fig.suptitle(f\"MTPINN Test Predictions - {city} - {mse_text}\")\n",
    "        axs.plot(city_data['norm_occ'], city_data['norm_flow'], 'o', c='k', alpha=0.5, label=\"True Flow\")\n",
    "        axs.plot(support_occupancy.view(-1), support_flow.view(-1), 'o', alpha=0.5, label=\"Support Flow\", c='orange')\n",
    "        axs.plot(occupancy_values, line_preds, label=\"Predicted Flow Line\", color='g', lw=2)\n",
    "        axs.axvline(query_cd.mean().item(), color='darkred', linestyle=':', lw=2, label=\"Critical Occupancy: \" + str(round(query_cd.mean().item(), 2)))\n",
    "        axs.axhline(query_max_flow.max().item(), color='darkred', linestyle='--', lw=2, label=\"Max Flow: \" + str(round(query_max_flow.max().item(), 2)))\n",
    "\n",
    "        axs.set_xlabel(\"Occupancy\", fontsize=16)\n",
    "        axs.set_ylabel(\"Flow\", fontsize=16)\n",
    "        axs.legend(fontsize=14)\n",
    "        plt.savefig(save_path + f'/{title_str}_mtpinn_do_{dropout}_{step}_{city}.png')\n",
    "        plt.show()\n",
    "\n",
    "    return mtpinn_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a8aa4",
   "metadata": {},
   "source": [
    "### The MAML Meta-Learning Model\n",
    "\n",
    "The code below contains functions for training and testing the meta-model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0365218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that trains the model using MAML with gradient accumulation over multiple tasks\n",
    "# and returns the trained meta-learner, test task loader, learned parameter history, and title string\n",
    "\n",
    "def meta_trainer(support_df,\n",
    "                 query_df,\n",
    "                 save_path,\n",
    "                 inner_steps=5,\n",
    "                 meta_iterations=100,\n",
    "                 lr_inner=0.01,\n",
    "                 lr_outer=0.001,\n",
    "                 k_shots=30,\n",
    "                 m_shots=100,\n",
    "                 accumulation_steps=3,\n",
    "                 step=\"\",\n",
    "                 dropout=0.0,\n",
    "                 detectors=75):\n",
    "    \n",
    "    # Set up path for saving figures:\n",
    "    save_path = f'{save_path}/meta_train/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Instantiate base model\n",
    "    model = MultiTaskPINNModel(dropout_prob=dropout)\n",
    "\n",
    "    # Prepare data: task loader yields tasks for support & query\n",
    "    task_loader, test_task_loader = prepare_data(\n",
    "        support_df, query_df,\n",
    "        k_shots, m_shots,\n",
    "        inner_steps=inner_steps)\n",
    "\n",
    "    # MAML wrapper and optimizers\n",
    "    maml = l2l.algorithms.MAML(model, lr=lr_inner)\n",
    "    meta_optimizer = optim.Adam(maml.parameters(), lr=lr_outer)\n",
    "\n",
    "    # Tracking\n",
    "    inner_losses = []\n",
    "    outer_losses = []\n",
    "    learned_params = {}\n",
    "\n",
    "    # Meta-training loop with accumulation\n",
    "    for iteration in range(meta_iterations):\n",
    "        # Zero meta-gradients at start of accumulation\n",
    "        meta_optimizer.zero_grad()\n",
    "\n",
    "        # Accumulate gradients over several tasks\n",
    "        for acc in range(accumulation_steps):\n",
    "            # Sample one task\n",
    "            task = next(iter(task_loader))\n",
    "            support = task['support']\n",
    "            query = task['query']\n",
    "\n",
    "            # Reshape support for inner loop\n",
    "            support_occupancy = support['occupancy'].view(inner_steps, k_shots, -1)\n",
    "            support_flow = support['flow'].view(inner_steps, k_shots, -1)\n",
    "\n",
    "            # Prepare query\n",
    "            query_occupancy = query['occupancy'].squeeze(0)\n",
    "            query_flow = query['flow'].squeeze(0)\n",
    "\n",
    "            # Clone for task-specific adaptation\n",
    "            learner = maml.clone()\n",
    "\n",
    "            # Inner-loop adaptation\n",
    "            for inner_batch in range(inner_steps):\n",
    "                batch_occ = support_occupancy[inner_batch].requires_grad_()\n",
    "                batch_flow = support_flow[inner_batch].requires_grad_()\n",
    "\n",
    "                preds, cd, max_flow, offset, occ_scaler = learner(batch_occ)\n",
    "                loss = learner.compute_total_loss(\n",
    "                    batch_flow, preds, cd, max_flow, offset, occ_scaler)\n",
    "                learner.adapt(loss, allow_unused=True)\n",
    "                inner_losses.append(loss.item())\n",
    "\n",
    "            # Evaluate on query\n",
    "            q_preds, q_cd, q_max_flow, q_offset, q_occ_scaler = learner(query_occupancy)\n",
    "            q_loss = learner.compute_total_loss(\n",
    "                query_flow, q_preds, q_cd, q_max_flow, q_offset, q_occ_scaler)\n",
    "            outer_losses.append(q_loss.item())\n",
    "\n",
    "            # Record parameter metrics\n",
    "            metrics = {\n",
    "                'cd': q_cd.mean().item(),\n",
    "                'max_flow': q_max_flow.max().item(),\n",
    "                'offset': q_offset.mean().item(),\n",
    "                'occ_scaler': q_occ_scaler.mean().item(),\n",
    "                'parabola1a': -(q_max_flow.mean().item() - q_offset.mean().item())\n",
    "                              / (q_cd.mean().item()**2 + 1e-6),\n",
    "                'parabola2a': -(q_max_flow.mean().item() - q_offset.mean().item())\n",
    "                              / (((q_occ_scaler.mean().item() - 1)*q_cd.mean().item())**2 + 1e-6)\n",
    "            }\n",
    "            for key, val in metrics.items():\n",
    "                learned_params.setdefault(key, []).append(val)\n",
    "\n",
    "            # Backward for meta-gradient accumulation\n",
    "            # Optionally normalize by accumulation_steps: q_loss = q_loss / accumulation_steps\n",
    "            (q_loss / accumulation_steps).backward()\n",
    "\n",
    "        # Meta-optimization step after accumulating\n",
    "        meta_optimizer.step()\n",
    "\n",
    "    # Plotting results\n",
    "    title_str = f'Meta-learning Experiment - detectors = {detectors} lr_i={lr_inner} lr_o={lr_outer} '\n",
    "    title_str += f'k={k_shots} m={m_shots} meta_iter={meta_iterations}'\n",
    "\n",
    "    fig_str = (\"Meta-learning Experiment - detectors={}, \"\n",
    "               \"alpha={}, beta={}, \"\n",
    "               \"k={}, m={}\").format(detectors, lr_inner, lr_outer, k_shots, m_shots)\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    fig.suptitle(fig_str, fontsize=16, fontweight='bold')\n",
    "    axs[0].plot(inner_losses, label='Inner Loop Loss')\n",
    "    axs[0].set_ylabel('Loss', fontsize=14)\n",
    "    axs[0].set_xlabel('Inner Iterations', fontsize=14)\n",
    "    axs[0].legend()\n",
    "    axs[1].plot(outer_losses, label='Meta Loss')\n",
    "    axs[1].set_ylabel('Loss', fontsize=14)\n",
    "    axs[1].set_xlabel('Accumulated Tasks', fontsize=14)\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.savefig(f\"{save_path}/{title_str}_acc{accumulation_steps}_do_{dropout}_{step}.png\")\n",
    "\n",
    "    return maml, test_task_loader, learned_params, title_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5809c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for meta-testing the model\n",
    "def meta_tester(meta_model, \n",
    "                query_df, \n",
    "                save_path, \n",
    "                support_occupancy, \n",
    "                support_flow, \n",
    "                query_occupancy, \n",
    "                query_flow, \n",
    "                inner_steps, \n",
    "                title_str, \n",
    "                city, \n",
    "                step, \n",
    "                dropout):\n",
    "    \n",
    "    # Set up path for saving figures:\n",
    "    save_path = f'{save_path}/meta_test/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Clone model for task-specific adaptation\n",
    "    learner = meta_model.clone()\n",
    "    mse_loss = nn.MSELoss()\n",
    "\n",
    "    meta_test_params = {}\n",
    "    inner_losses = []\n",
    "\n",
    "    # Inner-loop updates on support set (fine-tuning)\n",
    "    for inner_batch in range(inner_steps):\n",
    "        batch_occupancy = support_occupancy[inner_batch].requires_grad_()\n",
    "        batch_flow = support_flow[inner_batch].requires_grad_()\n",
    "        \n",
    "        support_predictions, pred_cd, pred_max_flow, pred_offset, pred_occ_scaler = learner(batch_occupancy)\n",
    "        support_loss = learner.compute_total_loss(batch_flow, support_predictions, pred_cd, pred_max_flow, pred_offset, pred_occ_scaler)\n",
    "        inner_losses.append(support_loss.item())\n",
    "        learner.adapt(support_loss)\n",
    "    \n",
    "    # plot the inner loop losses\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "    ax.plot(inner_losses, label=\"Inner Loop Loss\")\n",
    "    ax.set_ylabel(\"PI Total Loss\", fontsize=16)\n",
    "    ax.set_xlabel(\"Iteration\", fontsize=16)\n",
    "    ax.legend(fontsize=14)\n",
    "    plt.savefig(save_path + f'/{title_str}_inner_loss_do_{dropout}_{step}_{city}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate on the query set\n",
    "    with torch.no_grad(): # Disable gradients for query evaluation\n",
    "        query_predictions, query_cd, query_max_flow, query_offset, query_occ_scaler = learner(query_occupancy)\n",
    "        \n",
    "        # Compute MSE loss on the query set\n",
    "        query_loss = mse_loss(query_predictions, query_flow) # Compute loss on the query set\n",
    "\n",
    "        # Compute RRSE loss on the query set\n",
    "        query_flow_mean = query_flow.mean()\n",
    "        rrse_numerator = torch.sum((query_predictions - query_flow) ** 2)\n",
    "        rrse_denominator = torch.sum((query_flow - query_flow_mean) ** 2) + 1e-8  # Avoid division by zero\n",
    "        rrse = torch.sqrt(rrse_numerator / rrse_denominator)\n",
    "\n",
    "         # Compute correlation coefficient\n",
    "        query_predictions_mean = query_predictions.mean()\n",
    "        covariance = torch.sum((query_predictions - query_predictions_mean) * (query_flow - query_flow_mean))\n",
    "        variance_pred = torch.sum((query_predictions - query_predictions_mean) ** 2)\n",
    "        variance_true = torch.sum((query_flow - query_flow_mean) ** 2)\n",
    "        correlation_coefficient = covariance / (torch.sqrt(variance_pred * variance_true) + 1e-8)  # Avoid division by zero\n",
    "\n",
    "        # Compute the denormalized MSE, RRSE and correlation coefficient\n",
    "        query_predictions_denorm = denormalize(query_predictions, query_df['avg_flow'].min(), query_df['avg_flow'].max())\n",
    "        query_flow_denorm = denormalize(query_flow, query_df['avg_flow'].min(), query_df['avg_flow'].max())\n",
    "        query_loss_denorm = mse_loss(query_predictions_denorm, query_flow_denorm)\n",
    "        rrse_denorm = torch.sqrt(torch.sum((query_predictions_denorm - query_flow_denorm) ** 2) / torch.sum((query_flow_denorm - query_flow_denorm.mean()) ** 2))\n",
    "        covariance_denorm = torch.sum((query_predictions_denorm - query_predictions_denorm.mean()) * (query_flow_denorm - query_flow_denorm.mean()))\n",
    "        variance_pred_denorm = torch.sum((query_predictions_denorm - query_predictions_denorm.mean()) ** 2)\n",
    "        variance_true_denorm = torch.sum((query_flow_denorm - query_flow_denorm.mean()) ** 2)\n",
    "        correlation_coefficient_denorm = covariance_denorm / (torch.sqrt(variance_pred_denorm * variance_true_denorm) + 1e-8)  # Avoid division by zero\n",
    "\n",
    "\n",
    "\n",
    "        for key, value in {\n",
    "            'city': city,\n",
    "            'cd': query_cd.mean().item(),\n",
    "            'max_flow': query_max_flow.max().item(), # changed from mean to max!\n",
    "            'offset': query_offset.mean().item(),\n",
    "            'occ_scaler': query_occ_scaler.mean().item(),\n",
    "            'parabola1a': -(query_max_flow.mean().item() - query_offset.mean().item()) / (query_cd.mean().item() ** 2 + 1e-6),\n",
    "            'parabola2a': -(query_max_flow.mean().item() - query_offset.mean().item()) / (((query_occ_scaler.mean().item() - 1) * query_cd.mean().item()) ** 2 + 1e-6),\n",
    "            'mse loss': query_loss.item(),\n",
    "            'rrse': rrse.item(),\n",
    "            'correlation': correlation_coefficient.item(),\n",
    "            'mse loss denorm': query_loss_denorm.item(),\n",
    "            'rrse denorm': rrse_denorm.item(),\n",
    "            'correlation denorm': correlation_coefficient_denorm.item()\n",
    "        }.items():\n",
    "            meta_test_params.setdefault(key, []).append(value)\n",
    "\n",
    "        # Plot the query predictions - specify linearly spaced occupancy values for plotting the prediction\n",
    "        occupancy_values = torch.linspace(0, 1, 100).unsqueeze(1)\n",
    "        line_preds, _, _, _, _ = learner(occupancy_values)\n",
    "        city_data = query_df[query_df['city'] == city]\n",
    "        \n",
    "        mse_text = f\"MSE: {query_loss.item():.4f}\"\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(8, 5))\n",
    "        fig.suptitle(f\"Meta-Test Predictions - {city} - {mse_text}\", fontsize=16, fontweight='bold')\n",
    "        axs.plot(city_data['norm_occ'], city_data['norm_flow'], 'o', c='k', alpha=0.5, label=\"True Flow\")\n",
    "        axs.plot(support_occupancy.view(-1), support_flow.view(-1), 'o', alpha=0.5, label=\"Support Set\", color='orange')\n",
    "        axs.plot(occupancy_values, line_preds, label=\"Predicted Flow Line\", color='g', lw=2)\n",
    "        axs.axvline(query_cd.mean().item(), color='darkred', linestyle=':', lw=2, label=\"Critical Occupancy: \" + str(round(query_cd.mean().item(), 2)))\n",
    "        axs.axhline(query_max_flow.max().item(), color='darkred', linestyle='--', lw=2, label=\"Max Flow: \" + str(round(query_max_flow.max().item(), 2)))\n",
    "        \n",
    "        axs.set_xlabel(\"Occupancy\", fontsize=16)\n",
    "        axs.set_ylabel(\"Flow\", fontsize=16) \n",
    "        axs.legend(fontsize=14)\n",
    "        plt.savefig(save_path + f'/{title_str}_meta_test_do_{dropout}_{step}_{city}.png')\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"Meta-Test Query MSE-Loss: {query_loss.item():.4f}\")\n",
    "    \n",
    "    return meta_test_params \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ce6f3",
   "metadata": {},
   "source": [
    "### Setting up the experiment  \n",
    "\n",
    "An experiment is training the MAML model once, saving the training losses, the meta-test results and the results from the mtpinn model run for comparison. Each tine the experiment is run, 3 cities are chosen at random and left out of the training data passed to the meta-trainer function. These cities are then used for testing, and the results are stored in CSV files. When running multiple experiments, the test results are stored in the same CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e537abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up a function to \"run experiments\" i.e. execute the meta-training and testing with different hyperparameters\n",
    "def run_experiment(support_df, \n",
    "                   query_df, \n",
    "                   save_path, \n",
    "                   num_experiments=1, \n",
    "                   inner_steps=5, \n",
    "                   meta_iterations=100, \n",
    "                   lr_inner=0.01, \n",
    "                   lr_outer=0.001, \n",
    "                   k_shots=1000, \n",
    "                   m_shots=1000, \n",
    "                   dropout_maml=0.0, \n",
    "                   dropout_mtpinn=0.1, \n",
    "                   detectors=75):\n",
    "    \n",
    "    # Store results & parameters\n",
    "    meta_train_params = {}\n",
    "    meta_test_params = {}\n",
    "    mtpinn_params = {}\n",
    "\n",
    "    # Run the experiments\n",
    "    for n in range(num_experiments):\n",
    "        # Meta-training\n",
    "        maml_trained, test_loader, learned_params, title_str = meta_trainer(support_df, query_df, save_path, inner_steps=inner_steps, meta_iterations=meta_iterations, lr_inner=lr_inner, lr_outer=lr_outer, k_shots=k_shots, m_shots=m_shots, step=n, dropout=dropout_maml, detectors=detectors)\n",
    "  \n",
    "        for key, value in learned_params.items():\n",
    "            meta_train_params.setdefault(key, []).append(value)\n",
    "\n",
    "        # Meta-testing & MTPINN comparison\n",
    "        for task in test_loader:\n",
    "            city = task['city'][0]\n",
    "            support = task['support']\n",
    "            query = task['query']\n",
    "\n",
    "            support_occupancy = support['occupancy'].view(inner_steps, k_shots, -1)\n",
    "            support_flow = support['flow'].view(inner_steps, k_shots, -1)\n",
    "            query_occupancy = query['occupancy'].squeeze(0)\n",
    "            query_flow = query['flow'].squeeze(0)\n",
    "            \n",
    "            # Meta-test the MAML model\n",
    "            test_result = meta_tester(\n",
    "                maml_trained, query_df, save_path, support_occupancy, support_flow, query_occupancy, query_flow, inner_steps, title_str, city, n, dropout_maml\n",
    "            )\n",
    "            for key, value in test_result.items():\n",
    "                meta_test_params.setdefault(key, []).append(value)\n",
    "\n",
    "            # MTPINN Comparison\n",
    "            mtpinn_result = mtpinn_comparison(dropout_mtpinn, query_df, save_path, title_str, city, support_occupancy, support_flow, query_occupancy, query_flow, n\n",
    "            )\n",
    "            # Update meta_train_data dictionary\n",
    "            for key, value in mtpinn_result.items():\n",
    "                mtpinn_params.setdefault(key, []).append(value)\n",
    "\n",
    "    # Convert single-element lists to scalars\n",
    "    for key, value in meta_train_params.items():\n",
    "        if len(value) == 1:\n",
    "            # print(\"yep\")\n",
    "            meta_train_params[key] = value[0]\n",
    "\n",
    "    for key, value in meta_test_params.items():\n",
    "        meta_test_params[key] = [v[0] if isinstance(v, list) and len(v) == 1 else v for v in value]\n",
    "\n",
    "    for key, value in mtpinn_params.items():\n",
    "        mtpinn_params[key] = [v[0] if isinstance(v, list) and len(v) == 1 else v for v in value]\n",
    "\n",
    "    # Store results in a dataframe\n",
    "    meta_train_df = pd.DataFrame(meta_train_params)\n",
    "    meta_test_df = pd.DataFrame(meta_test_params)\n",
    "    mtpinn_df = pd.DataFrame(mtpinn_params)\n",
    "\n",
    "    # Ensure column 'city' is a string\n",
    "    meta_test_df['city'] = meta_test_df['city'].astype(str)\n",
    "    mtpinn_df['city'] = mtpinn_df['city'].astype(str)\n",
    "\n",
    "    # Save the results as CSV\n",
    "    meta_train_df.to_csv(f'{save_path}meta_train_results_{title_str}_.csv', index=False)\n",
    "    meta_test_df.to_csv(f'{save_path}meta_test_results_{title_str}_.csv', index=False)\n",
    "    mtpinn_df.to_csv(f'{save_path}mtpinn_results_{title_str}_.csv', index=False)\n",
    "    return meta_train_df, meta_test_df, mtpinn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ace50",
   "metadata": {},
   "source": [
    "### Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc6d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the experiment for 75 detectors, 1000 k_shots and 1000 m_shots\n",
    "number_experiments = 5      # number of experiments to run, i.e. MAML models to train\n",
    "inner_steps = 5             # number of inner steps for MAML\n",
    "meta_iterations = 150       # number of meta-iterations\n",
    "lr_inner = 0.02             # inner loop learning rate\n",
    "lr_outer = 0.005            # outer loop learning rate \n",
    "k_shots = 150               # number of support samples per task, per inner step\n",
    "m_shots = 750               # number of query samples per task\n",
    "dropout_maml = 0.0          # dropout for MAML is always set to 0.0 (as dropout was seen to decrease performance)\n",
    "dropout_mtpinn = 0.1        # dropout for MTPINN is set to 0.1 (as dropout was seen to increase performance)\n",
    "\n",
    "base_path = 'C:/DTU/Speciale/Kode/Speciale/meta-learning/test_save_path/'\n",
    "save_path = f'{base_path}/75_detectors/'\n",
    "\n",
    "meta_train_params, meta_test_params, mtpinn_params = run_experiment(support_df=support_75, query_df=query_df, save_path=save_path, num_experiments=5, inner_steps=5, meta_iterations=150, lr_inner=0.02, lr_outer=0.005, k_shots=150, m_shots=750, dropout_maml=0.0, dropout_mtpinn=0.1, detectors=75) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40973b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the experiment for 50 detectors, 1000 k_shots and 1000 m_shots\n",
    "save_path = f'{base_path}/50_detectors/'\n",
    "\n",
    "meta_train_params, meta_test_params, mtpinn_params = run_experiment(support_df=support_50, query_df=query_df, save_path=save_path, num_experiments=5, inner_steps=5, meta_iterations=150, lr_inner=0.02, lr_outer=0.005, k_shots=150, m_shots=750, dropout_maml=0.0, dropout_mtpinn=0.1, detectors=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75588520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the experiment for 25 detectors, 1000 k_shots and 1000 m_shots\n",
    "save_path = f'{base_path}/25_detectors/'\n",
    "\n",
    "meta_train_params, meta_test_params, mtpinn_params = run_experiment(support_df=support_25, query_df=query_df, save_path=save_path, num_experiments=5, inner_steps=5, meta_iterations=150, lr_inner=0.02, lr_outer=0.005, k_shots=150, m_shots=750, dropout_maml=0.0, dropout_mtpinn=0.1, detectors=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b24181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
